{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06ae4dc",
   "metadata": {},
   "source": [
    "# MHA Toolbox: Metaheuristic Algorithm Toolbox Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the MHA Toolbox, a flexible and user-friendly library for metaheuristic optimization algorithms.\n",
    "\n",
    "The toolbox features:\n",
    "- Automatic parameter handling (missing parameters are automatically calculated)\n",
    "- Unified interface across all algorithms\n",
    "- Support for both direct optimization and feature selection\n",
    "- Comprehensive results objects with built-in analysis tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34ef57",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import MHA toolbox\n",
    "from mha_toolbox.toolbox import get_optimizer, list_algorithms\n",
    "import mha_toolbox.benchmarks as benchmarks\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6265d64",
   "metadata": {},
   "source": [
    "## 2. List Available Algorithms\n",
    "\n",
    "First, let's check what algorithms are available in the toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available algorithms\n",
    "print(\"Available algorithms:\")\n",
    "print(list_algorithms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9118cf6",
   "metadata": {},
   "source": [
    "## 3. Basic Usage: Optimizing a Benchmark Function\n",
    "\n",
    "Let's start with a simple example: optimizing the Sphere function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function (using built-in benchmark)\n",
    "sphere = benchmarks.sphere\n",
    "\n",
    "# Create SCA optimizer with full parameter specification\n",
    "optimizer = get_optimizer(\n",
    "    \"SCA\",\n",
    "    dimensions=10,\n",
    "    lower_bound=-5,\n",
    "    upper_bound=5,\n",
    "    population_size=30,\n",
    "    max_iterations=100,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result = optimizer.optimize(objective_function=sphere)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest fitness: {result.best_fitness}\")\n",
    "print(f\"Execution time: {result.execution_time:.4f} seconds\")\n",
    "\n",
    "# Plot convergence curve\n",
    "result.plot_convergence(title=\"Sphere Function Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d9f0d",
   "metadata": {},
   "source": [
    "## 4. Automatic Parameter Handling\n",
    "\n",
    "Now let's see how the toolbox handles missing parameters by providing only some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another benchmark function\n",
    "rastrigin = benchmarks.rastrigin\n",
    "\n",
    "# Create optimizer with partial parameters\n",
    "# Notice we only provide dimensions and upper_bound\n",
    "optimizer = get_optimizer(\n",
    "    \"SCA\",\n",
    "    dimensions=5,\n",
    "    upper_bound=5.12,  # Only specify upper bound\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result = optimizer.optimize(objective_function=rastrigin)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest fitness: {result.best_fitness}\")\n",
    "\n",
    "# Show the parameters that were actually used (including derived ones)\n",
    "print(\"\\nParameters used:\")\n",
    "for key, value in result.parameters.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "# Plot convergence curve\n",
    "result.plot_convergence(title=\"Rastrigin Function Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03067059",
   "metadata": {},
   "source": [
    "## 5. Feature Selection Example\n",
    "\n",
    "The toolbox can also be used for feature selection. Let's use a real dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Feature names: {data.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer for feature selection\n",
    "# Notice we don't specify any bounds or dimensions - they'll be derived from the data\n",
    "optimizer = get_optimizer(\n",
    "    \"SCA\",\n",
    "    population_size=20,\n",
    "    max_iterations=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run feature selection\n",
    "fs_result = optimizer.optimize(X=X, y=y)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest fitness (error rate): {fs_result.best_fitness}\")\n",
    "\n",
    "# Get selected features\n",
    "selected_indices = np.where(fs_result.best_solution_binary)[0]\n",
    "selected_names = [data.feature_names[i] for i in selected_indices]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_indices)} out of {X.shape[1]} features:\")\n",
    "for i, (idx, name) in enumerate(zip(selected_indices, selected_names)):\n",
    "    print(f\"  {i+1}. Feature {idx}: {name}\")\n",
    "\n",
    "# Plot convergence curve\n",
    "fs_result.plot_convergence(title=\"Feature Selection Convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e379909",
   "metadata": {},
   "source": [
    "## 6. Evaluating Feature Selection Results\n",
    "\n",
    "Let's evaluate how good our feature selection is by training a classifier with the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = np.where(fs_result.best_solution_binary)[0]\n",
    "X_selected = X[:, selected_features]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a classifier on the selected features\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy with {len(selected_features)} selected features: {accuracy:.4f}\")\n",
    "\n",
    "# Compare with using all features\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf_full = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_full.fit(X_train_full, y_train_full)\n",
    "y_pred_full = clf_full.predict(X_test_full)\n",
    "accuracy_full = accuracy_score(y_test_full, y_pred_full)\n",
    "\n",
    "print(f\"Accuracy with all {X.shape[1]} features: {accuracy_full:.4f}\")\n",
    "print(f\"Feature reduction: {100 * (1 - len(selected_features) / X.shape[1]):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f17e0",
   "metadata": {},
   "source": [
    "## 7. Customizing Objective Functions\n",
    "\n",
    "You can also define your own objective functions for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom objective function\n",
    "def custom_objective(solution):\n",
    "    \"\"\"\n",
    "    Ackley function - a non-convex optimization benchmark function\n",
    "    \"\"\"\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2 * np.pi\n",
    "    n = len(solution)\n",
    "    \n",
    "    sum1 = np.sum(solution**2)\n",
    "    sum2 = np.sum(np.cos(c * solution))\n",
    "    \n",
    "    term1 = -a * np.exp(-b * np.sqrt(sum1 / n))\n",
    "    term2 = -np.exp(sum2 / n)\n",
    "    \n",
    "    return term1 + term2 + a + np.exp(1)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = get_optimizer(\n",
    "    \"SCA\",\n",
    "    dimensions=10,\n",
    "    lower_bound=-32.768,\n",
    "    upper_bound=32.768,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result = optimizer.optimize(objective_function=custom_objective)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest fitness: {result.best_fitness}\")\n",
    "print(f\"Best solution: {result.best_solution[:5]}...\")\n",
    "result.plot_convergence(title=\"Custom Objective Function Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c20cde",
   "metadata": {},
   "source": [
    "## 8. Exploring the Model Object\n",
    "\n",
    "The result model object contains comprehensive information about the optimization run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44905ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the optimization results\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a870f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual attributes\n",
    "print(\"Accessing individual attributes:\")\n",
    "print(f\"Algorithm: {result.algorithm_name}\")\n",
    "print(f\"Best fitness: {result.best_fitness}\")\n",
    "print(f\"Execution time: {result.execution_time:.4f} seconds\")\n",
    "print(f\"Population size: {result.parameters['population_size']}\")\n",
    "print(f\"\\nConvergence curve (first 5 iterations):\")\n",
    "for i, fitness in enumerate(result.convergence_curve[:5]):\n",
    "    print(f\"Iteration {i+1}: {fitness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a62722",
   "metadata": {},
   "source": [
    "## 9. Default Parameter Demonstration\n",
    "\n",
    "Let's see what happens when we provide no parameters at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe439d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer with no parameters\n",
    "optimizer = get_optimizer(\"SCA\", verbose=True)\n",
    "\n",
    "# Define a simple test function\n",
    "def simple_function(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# Run optimization\n",
    "result = optimizer.optimize(objective_function=simple_function)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nParameters that were automatically set:\")\n",
    "for key, value in result.parameters.items():\n",
    "    if key not in ['algorithm_name']:\n",
    "        print(f\"  - {key}: {value}\")\n",
    "\n",
    "# Plot convergence curve\n",
    "result.plot_convergence(title=\"Optimization with Default Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a8afa",
   "metadata": {},
   "source": [
    "## 10. Working with Data from Files\n",
    "\n",
    "Let's see how to use the toolbox with data loaded from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a sample dataset and save it\n",
    "X_sample, y_sample = make_classification(n_samples=100, n_features=20, \n",
    "                                        n_informative=5, n_redundant=10,\n",
    "                                        random_state=42)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features = [f\"feature_{i}\" for i in range(X_sample.shape[1])]\n",
    "df = pd.DataFrame(X_sample, columns=features)\n",
    "df['target'] = y_sample\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"sample_dataset.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"Sample dataset saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "X = data.drop(columns=['target']).values\n",
    "y = data['target'].values\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = get_optimizer(\n",
    "    \"SCA\",\n",
    "    population_size=20,\n",
    "    max_iterations=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run feature selection\n",
    "result = optimizer.optimize(X=X, y=y)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest fitness: {result.best_fitness}\")\n",
    "print(f\"Selected {sum(result.best_solution_binary)} out of {X.shape[1]} features\")\n",
    "print(f\"Selected features: {np.where(result.best_solution_binary)[0]}\")\n",
    "result.plot_convergence(title=\"Feature Selection on Loaded Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03eb86",
   "metadata": {},
   "source": [
    "## 11. Tips and Best Practices\n",
    "\n",
    "### Setting Appropriate Bounds\n",
    "\n",
    "- For benchmark functions, use the known bounds of the function\n",
    "- For feature selection, the toolbox automatically uses [0, 1] bounds\n",
    "- If you know the bounds for only one side, the other will be intelligently derived\n",
    "\n",
    "### Choosing Population Size and Iterations\n",
    "\n",
    "- For simple problems, small populations (10-30) are often sufficient\n",
    "- For complex problems with many dimensions, use larger populations (50-100)\n",
    "- The number of iterations typically scales with problem complexity\n",
    "- For feature selection, a good starting point is 10Ã— the number of features for iterations\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "- Check the convergence curve to see if optimization has stabilized\n",
    "- For feature selection, compare accuracy with selected features vs. all features\n",
    "- Remember that the goal is to minimize the fitness function\n",
    "\n",
    "### Custom Objective Functions\n",
    "\n",
    "- Ensure your function is vectorized for better performance\n",
    "- The function should accept a solution vector and return a single fitness value\n",
    "- Lower fitness values are considered better (the toolbox minimizes the objective)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
